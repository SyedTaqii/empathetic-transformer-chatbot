{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-10-16T05:39:50.581531Z",
     "iopub.status.busy": "2025-10-16T05:39:50.581254Z",
     "iopub.status.idle": "2025-10-16T05:39:51.915599Z",
     "shell.execute_reply": "2025-10-16T05:39:51.914767Z",
     "shell.execute_reply.started": "2025-10-16T05:39:50.581506Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading the dataset...\n",
      "Successfully loaded 64636 total dialogues.\n",
      "Extracting customer utterances...\n",
      "\n",
      "Sample of processed data:\n",
      "                                           Situation      emotion  \\\n",
      "0  I remember going to the fireworks with my best...  sentimental   \n",
      "1  I remember going to the fireworks with my best...  sentimental   \n",
      "2  I remember going to the fireworks with my best...  sentimental   \n",
      "3  I remember going to the fireworks with my best...  sentimental   \n",
      "4  I remember going to the fireworks with my best...  sentimental   \n",
      "\n",
      "  customer_utterance                                        agent_reply  \n",
      "0            Agent :  Was this a friend you were in love with, or ju...  \n",
      "1            Agent :                                Where has she gone?  \n",
      "2            Agent :  Oh was this something that happened because of...  \n",
      "3            Agent :                This was a best friend. I miss her.  \n",
      "4            Agent :                                 We no longer talk.  \n",
      "\n",
      "--- Dataset Split ---\n",
      "Training set size:   51708 (80%)\n",
      "Validation set size: 6464 (10%)\n",
      "Test set size:       6464 (10%)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 1. Define the file path from your Kaggle input directory\n",
    "# Make sure the path matches what you see in the \"Input\" section of your notebook\n",
    "file_path = '/home/ununtu1-5090/Documents/Abdullah/taqi/emotion-emotion_69k.csv'\n",
    "\n",
    "# 2. Load the dataset into a pandas DataFrame\n",
    "print(\"Loading the dataset...\")\n",
    "df = pd.read_csv(file_path)\n",
    "print(f\"Successfully loaded {len(df)} total dialogues.\")\n",
    "\n",
    "# 3. Clean up the data by parsing the customer utterance\n",
    "# The 'empathetic_dialogues' column contains both customer and agent parts.\n",
    "# We need to extract just the customer's line.\n",
    "def extract_customer_utterance(dialogue):\n",
    "    # Find the text after \"Customer :\" and before the final \"Agent :\"\n",
    "    try:\n",
    "        # The prompt is everything after the first newline\n",
    "        return dialogue.split('\\n')[1].replace('Customer :', '').strip()\n",
    "    except IndexError:\n",
    "        # Handle cases where the format might be different or empty\n",
    "        return \"\"\n",
    "\n",
    "print(\"Extracting customer utterances...\")\n",
    "df['customer_utterance'] = df['empathetic_dialogues'].apply(extract_customer_utterance)\n",
    "\n",
    "# Let's rename the 'labels' column to be more intuitive\n",
    "df.rename(columns={'labels': 'agent_reply'}, inplace=True)\n",
    "\n",
    "# Display a sample to verify\n",
    "print(\"\\nSample of processed data:\")\n",
    "print(df[['Situation', 'emotion', 'customer_utterance', 'agent_reply']].head())\n",
    "\n",
    "\n",
    "# 4. Split the data into 80% train, 10% validation, and 10% test \n",
    "# First, split into 80% training and 20% temporary (for validation + test)\n",
    "train_df, temp_df = train_test_split(\n",
    "    df,\n",
    "    test_size=0.2,      # 20% for temp_df\n",
    "    random_state=42     # A fixed random state ensures the split is the same every time\n",
    ")\n",
    "\n",
    "# Now, split the temporary set into two equal halves (10% validation, 10% test)\n",
    "val_df, test_df = train_test_split(\n",
    "    temp_df,\n",
    "    test_size=0.5,      # 50% of the 20% is 10% of the total\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# 5. Verify the size of each split\n",
    "print(\"\\n--- Dataset Split ---\")\n",
    "print(f\"Training set size:   {len(train_df)} ({len(train_df)/len(df):.0%})\")\n",
    "print(f\"Validation set size: {len(val_df)} ({len(val_df)/len(df):.0%})\")\n",
    "print(f\"Test set size:       {len(test_df)} ({len(test_df)/len(df):.0%})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:39:51.917355Z",
     "iopub.status.busy": "2025-10-16T05:39:51.916972Z",
     "iopub.status.idle": "2025-10-16T05:39:53.560993Z",
     "shell.execute_reply": "2025-10-16T05:39:53.560326Z",
     "shell.execute_reply.started": "2025-10-16T05:39:51.917332Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Normalizing text in all data splits...\n",
      "Normalization complete.\n",
      "\n",
      "Sample of normalized training data:\n",
      "                                               Situation customer_utterance  \\\n",
      "41481  i had to go buy legos for my nephew the other ...            agent :   \n",
      "52816                   i have a hot date this weekend !            agent :   \n",
      "31326               i still believe he can make me proud            agent :   \n",
      "16889  i didn ' t ask the girl i like to the prom and...            agent :   \n",
      "18465  i was impressed at this workshop i went to ove...            agent :   \n",
      "\n",
      "                                             agent_reply  \n",
      "41481  no just this feeling overcame me that my kids ...  \n",
      "52816      i am taking to a movie and the olive garden .  \n",
      "31326                oh nice . what type of presentation  \n",
      "16889  if you could do it all over again , how would ...  \n",
      "18465                                 ok cool how was it  \n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"\n",
    "    Performs text normalization:\n",
    "    1. Lowercases the text.\n",
    "    2. Adds space around punctuation.\n",
    "    3. Removes extra whitespace.\n",
    "    \"\"\"\n",
    "    text = text.lower()\n",
    "    # Add a space between words and punctuation marks.\n",
    "    text = re.sub(f\"([{string.punctuation}])\", r\" \\1 \", text)\n",
    "    # Collapse multiple spaces into a single space and strip leading/trailing whitespace.\n",
    "    text = re.sub(r\"\\s+\", \" \", text).strip()\n",
    "    return text\n",
    "\n",
    "# The columns we need to clean\n",
    "text_columns = ['Situation', 'customer_utterance', 'agent_reply']\n",
    "\n",
    "print(\"Normalizing text in all data splits...\")\n",
    "\n",
    "# Apply the normalization function to each dataframe\n",
    "for df in [train_df, val_df, test_df]:\n",
    "    for col in text_columns:\n",
    "        df[col] = df[col].apply(normalize_text)\n",
    "\n",
    "print(\"Normalization complete.\")\n",
    "\n",
    "# Display a sample from the training set to see the result\n",
    "print(\"\\nSample of normalized training data:\")\n",
    "print(train_df[['Situation', 'customer_utterance', 'agent_reply']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:39:53.562029Z",
     "iopub.status.busy": "2025-10-16T05:39:53.561719Z",
     "iopub.status.idle": "2025-10-16T05:39:55.184532Z",
     "shell.execute_reply": "2025-10-16T05:39:55.183853Z",
     "shell.execute_reply.started": "2025-10-16T05:39:53.561989Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the tokenizer...\n",
      "\n",
      "\n",
      "\n",
      "Tokenizer saved to ./tokenizer\n",
      "\n",
      "--- Tokenizer Test ---\n",
      "Original: i remember going to the fireworks .\n",
      "Tokens: ['[CLS]', 'i', 'remember', 'going', 'to', 'the', 'fireworks', '.', '[SEP]']\n",
      "IDs: [2, 51, 750, 274, 121, 124, 6036, 19, 3]\n"
     ]
    }
   ],
   "source": [
    "# # 1. Install the tokenizers library (if needed)\n",
    "!pip install tokenizers -q\n",
    "\n",
    "import os\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "# Create a directory to save the tokenizer\n",
    "tokenizer_dir = \"./tokenizer\"\n",
    "if not os.path.exists(tokenizer_dir):\n",
    "    os.makedirs(tokenizer_dir)\n",
    "\n",
    "# 2. Prepare an iterator from our training data\n",
    "text_iterator = (\n",
    "    row\n",
    "    for row in train_df['Situation']\n",
    "    + train_df['customer_utterance']\n",
    "    + train_df['agent_reply']\n",
    ")\n",
    "\n",
    "# 3. Initialize and train the tokenizer\n",
    "custom_tokenizer = BertWordPieceTokenizer(\n",
    "    clean_text=False,\n",
    "    handle_chinese_chars=False,\n",
    "    strip_accents=False,\n",
    "    lowercase=False\n",
    ")\n",
    "\n",
    "print(\"Training the tokenizer...\")\n",
    "# --- FIX IS HERE ---\n",
    "# Add the standard BERT special tokens ([CLS], [SEP]) along with the ones we need.\n",
    "custom_tokenizer.train_from_iterator(\n",
    "    text_iterator,\n",
    "    vocab_size=10000,\n",
    "    min_frequency=2,\n",
    "    special_tokens=[\"[PAD]\", \"[UNK]\", \"[CLS]\", \"[SEP]\", \"[BOS]\", \"[EOS]\"] # Added [CLS] and [SEP]\n",
    ")\n",
    "\n",
    "# 4. Save the tokenizer files\n",
    "custom_tokenizer.save_model(tokenizer_dir)\n",
    "print(f\"Tokenizer saved to {tokenizer_dir}\")\n",
    "\n",
    "# 5. Load the tokenizer and test it\n",
    "# This part will now work without errors\n",
    "tokenizer_path = os.path.join(tokenizer_dir, \"vocab.txt\")\n",
    "tokenizer = BertWordPieceTokenizer(tokenizer_path, lowercase=False)\n",
    "\n",
    "# Test with a sample sentence\n",
    "sample_text = \"i remember going to the fireworks .\"\n",
    "encoded = tokenizer.encode(sample_text)\n",
    "\n",
    "print(\"\\n--- Tokenizer Test ---\")\n",
    "print(f\"Original: {sample_text}\")\n",
    "print(f\"Tokens: {encoded.tokens}\")\n",
    "print(f\"IDs: {encoded.ids}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:39:55.186681Z",
     "iopub.status.busy": "2025-10-16T05:39:55.186285Z",
     "iopub.status.idle": "2025-10-16T05:39:58.777390Z",
     "shell.execute_reply": "2025-10-16T05:39:58.776554Z",
     "shell.execute_reply.started": "2025-10-16T05:39:55.186662Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating Datasets and DataLoaders...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 70\u001b[0m\n\u001b[0;32m     68\u001b[0m \u001b[38;5;66;03m# 4. Instantiate Datasets and DataLoaders\u001b[39;00m\n\u001b[0;32m     69\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating Datasets and DataLoaders...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 70\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m EmpatheticDialoguesDataset(\u001b[43mtrain_df\u001b[49m, tokenizer)\n\u001b[0;32m     71\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m EmpatheticDialoguesDataset(val_df, tokenizer)\n\u001b[0;32m     73\u001b[0m BATCH_SIZE \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m32\u001b[39m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'train_df' is not defined"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "from tokenizers import BertWordPieceTokenizer\n",
    "\n",
    "# 1. Load the trained tokenizer from the saved file\n",
    "tokenizer_path = \"./tokenizer/vocab.txt\"\n",
    "tokenizer = BertWordPieceTokenizer(tokenizer_path, lowercase=False)\n",
    "\n",
    "# Define special token IDs\n",
    "pad_token_id = tokenizer.token_to_id('[PAD]')\n",
    "bos_token_id = tokenizer.token_to_id('[BOS]')\n",
    "eos_token_id = tokenizer.token_to_id('[EOS]')\n",
    "\n",
    "# 2. Create the custom PyTorch Dataset\n",
    "class EmpatheticDialoguesDataset(Dataset):\n",
    "    def __init__(self, dataframe, tokenizer, max_len=128):\n",
    "        self.dataframe = dataframe\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.dataframe)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        row = self.dataframe.iloc[idx]\n",
    "\n",
    "        # Format the input and target strings\n",
    "        input_text = f\"Emotion: {row['emotion']} | Situation: {row['Situation']} | Customer: {row['customer_utterance']} Agent:\"\n",
    "        target_text = row['agent_reply']\n",
    "\n",
    "        # Tokenize the texts\n",
    "        input_encoding = self.tokenizer.encode(input_text)\n",
    "        target_encoding = self.tokenizer.encode(target_text)\n",
    "\n",
    "        # Prepare model inputs\n",
    "        encoder_input = [bos_token_id] + input_encoding.ids + [eos_token_id]\n",
    "        decoder_input = [bos_token_id] + target_encoding.ids\n",
    "        label = target_encoding.ids + [eos_token_id]\n",
    "\n",
    "        # Truncate if necessary\n",
    "        encoder_input = encoder_input[:self.max_len]\n",
    "        decoder_input = decoder_input[:self.max_len-1]\n",
    "        label = label[:self.max_len-1]\n",
    "\n",
    "        return {\n",
    "            \"encoder_input\": torch.tensor(encoder_input, dtype=torch.long),\n",
    "            \"decoder_input\": torch.tensor(decoder_input, dtype=torch.long),\n",
    "            \"label\": torch.tensor(label, dtype=torch.long)\n",
    "        }\n",
    "\n",
    "# 3. Create a collate function for padding\n",
    "def collate_fn(batch):\n",
    "    encoder_inputs = [item['encoder_input'] for item in batch]\n",
    "    decoder_inputs = [item['decoder_input'] for item in batch]\n",
    "    labels = [item['label'] for item in batch]\n",
    "\n",
    "    encoder_inputs_padded = pad_sequence(encoder_inputs, batch_first=True, padding_value=pad_token_id)\n",
    "    decoder_inputs_padded = pad_sequence(decoder_inputs, batch_first=True, padding_value=pad_token_id)\n",
    "    labels_padded = pad_sequence(labels, batch_first=True, padding_value=pad_token_id)\n",
    "\n",
    "    return {\n",
    "        \"encoder_input\": encoder_inputs_padded,\n",
    "        \"decoder_input\": decoder_inputs_padded,\n",
    "        \"label\": labels_padded\n",
    "    }\n",
    "\n",
    "# 4. Instantiate Datasets and DataLoaders\n",
    "print(\"Creating Datasets and DataLoaders...\")\n",
    "train_dataset = EmpatheticDialoguesDataset(train_df, tokenizer)\n",
    "val_dataset = EmpatheticDialoguesDataset(val_df, tokenizer)\n",
    "\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=BATCH_SIZE, shuffle=False, collate_fn=collate_fn)\n",
    "\n",
    "print(\"DataLoaders are ready.\")\n",
    "\n",
    "# 5. Test the DataLoader\n",
    "print(\"\\n--- Testing one batch from the train_dataloader ---\")\n",
    "sample_batch = next(iter(train_dataloader))\n",
    "print(\"Encoder input shape:\", sample_batch['encoder_input'].shape)\n",
    "print(\"Decoder input shape:\", sample_batch['decoder_input'].shape)\n",
    "print(\"Label shape:\", sample_batch['label'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:39:58.778793Z",
     "iopub.status.busy": "2025-10-16T05:39:58.778313Z",
     "iopub.status.idle": "2025-10-16T05:39:58.785993Z",
     "shell.execute_reply": "2025-10-16T05:39:58.785243Z",
     "shell.execute_reply.started": "2025-10-16T05:39:58.778774Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PositionalEncoding class defined.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, max_len=5000, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the PositionalEncoding module.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The embedding dimension of the model.\n",
    "            max_len (int): The maximum possible length of a sequence.\n",
    "            dropout (float): The dropout rate.\n",
    "        \"\"\"\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        # Create a positional encoding matrix of shape (max_len, d_model)\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "\n",
    "        # Create a position tensor of shape (max_len, 1)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "\n",
    "        # Calculate the division term for the sine and cosine functions\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2).float() * (-math.log(10000.0) / d_model))\n",
    "\n",
    "        # Apply sine to even indices in the array; 2i\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        # Apply cosine to odd indices in the array; 2i+1\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "\n",
    "        # Add a batch dimension to the positional encoding matrix so it can be added to the input embeddings\n",
    "        pe = pe.unsqueeze(0) # Shape: (1, max_len, d_model)\n",
    "\n",
    "        # Register 'pe' as a buffer. Buffers are part of the model's state,\n",
    "        # but they are not considered parameters to be trained.\n",
    "        self.register_buffer('pe', pe)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Adds positional encoding to the input tensor.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor (embeddings) of shape (batch_size, seq_len, d_model).\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor with positional information added.\n",
    "        \"\"\"\n",
    "        # Add the positional encoding to the input embeddings.\n",
    "        # x.size(1) is the sequence length of the current batch.\n",
    "        x = x + self.pe[:, :x.size(1), :]\n",
    "        return self.dropout(x)\n",
    "\n",
    "print(\"PositionalEncoding class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:39:58.787368Z",
     "iopub.status.busy": "2025-10-16T05:39:58.786676Z",
     "iopub.status.idle": "2025-10-16T05:39:58.809886Z",
     "shell.execute_reply": "2025-10-16T05:39:58.809166Z",
     "shell.execute_reply.started": "2025-10-16T05:39:58.787346Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MultiHeadAttention class defined.\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class MultiHeadAttention(nn.Module):\n",
    "    def __init__(self, d_model, h, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the MultiHeadAttention module.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The embedding dimension of the model.\n",
    "            h (int): The number of attention heads.\n",
    "            dropout (float): The dropout rate.\n",
    "        \"\"\"\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        assert d_model % h == 0, \"d_model must be divisible by h\"\n",
    "\n",
    "        self.d_model = d_model\n",
    "        self.h = h\n",
    "        self.d_k = d_model // h # Dimension of keys/queries/values per head\n",
    "\n",
    "        # Linear layers for Query, Key, Value, and the final output\n",
    "        self.w_q = nn.Linear(d_model, d_model)\n",
    "        self.w_k = nn.Linear(d_model, d_model)\n",
    "        self.w_v = nn.Linear(d_model, d_model)\n",
    "        self.w_o = nn.Linear(d_model, d_model)\n",
    "\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        \"\"\"\n",
    "        Performs the forward pass for multi-head attention.\n",
    "\n",
    "        Args:\n",
    "            query (torch.Tensor): Query tensor, shape (batch_size, seq_len_q, d_model)\n",
    "            key (torch.Tensor): Key tensor, shape (batch_size, seq_len_k, d_model)\n",
    "            value (torch.Tensor): Value tensor, shape (batch_size, seq_len_v, d_model)\n",
    "            mask (torch.Tensor, optional): Mask to prevent attention to certain positions.\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output of the multi-head attention, shape (batch_size, seq_len_q, d_model)\n",
    "        \"\"\"\n",
    "        batch_size = query.size(0)\n",
    "\n",
    "        # 1. Linearly project and split into h heads\n",
    "        q = self.w_q(query).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        k = self.w_k(key).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        v = self.w_v(value).view(batch_size, -1, self.h, self.d_k).transpose(1, 2)\n",
    "        # Shape of q, k, v is now (batch_size, h, seq_len, d_k)\n",
    "\n",
    "        # 2. Calculate attention scores\n",
    "        scores = torch.matmul(q, k.transpose(-2, -1)) / math.sqrt(self.d_k)\n",
    "\n",
    "        # 3. Apply mask (if provided)\n",
    "        if mask is not None:\n",
    "            # The mask needs to be broadcastable to the scores shape\n",
    "            scores = scores.masked_fill(mask == 0, -1e9)\n",
    "\n",
    "        # 4. Apply softmax to get attention weights\n",
    "        attention_weights = F.softmax(scores, dim=-1)\n",
    "        attention_weights = self.dropout(attention_weights)\n",
    "\n",
    "        # 5. Multiply weights by values\n",
    "        context = torch.matmul(attention_weights, v) # (batch_size, h, seq_len_q, d_k)\n",
    "\n",
    "        # 6. Concatenate heads and apply final linear layer\n",
    "        context = context.transpose(1, 2).contiguous().view(batch_size, -1, self.d_model)\n",
    "        output = self.w_o(context)\n",
    "\n",
    "        return output\n",
    "\n",
    "print(\"MultiHeadAttention class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:39:58.811046Z",
     "iopub.status.busy": "2025-10-16T05:39:58.810743Z",
     "iopub.status.idle": "2025-10-16T05:39:58.832289Z",
     "shell.execute_reply": "2025-10-16T05:39:58.831543Z",
     "shell.execute_reply.started": "2025-10-16T05:39:58.811024Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PositionwiseFeedForward class defined.\n"
     ]
    }
   ],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        \"\"\"\n",
    "        Initializes the PositionwiseFeedForward module.\n",
    "\n",
    "        Args:\n",
    "            d_model (int): The embedding dimension of the model.\n",
    "            d_ff (int): The dimension of the inner feed-forward layer.\n",
    "                        A common value is d_model * 4.\n",
    "            dropout (float): The dropout rate.\n",
    "        \"\"\"\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.linear1 = nn.Linear(d_model, d_ff)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.linear2 = nn.Linear(d_ff, d_model)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"\n",
    "        Performs the forward pass.\n",
    "\n",
    "        Args:\n",
    "            x (torch.Tensor): The input tensor, shape (batch_size, seq_len, d_model)\n",
    "\n",
    "        Returns:\n",
    "            torch.Tensor: The output tensor, shape (batch_size, seq_len, d_model)\n",
    "        \"\"\"\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = self.dropout(x)\n",
    "        x = self.linear2(x)\n",
    "        return x\n",
    "\n",
    "print(\"PositionwiseFeedForward class defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:39:58.833545Z",
     "iopub.status.busy": "2025-10-16T05:39:58.833225Z",
     "iopub.status.idle": "2025-10-16T05:39:58.846722Z",
     "shell.execute_reply": "2025-10-16T05:39:58.845914Z",
     "shell.execute_reply.started": "2025-10-16T05:39:58.833522Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EncoderLayer and DecoderLayer classes defined.\n"
     ]
    }
   ],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, h, d_ff, dropout):\n",
    "        \"\"\"\n",
    "        Initializes a single EncoderLayer.\n",
    "        \"\"\"\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, h, dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, src, src_mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the EncoderLayer.\n",
    "        src: input tensor (batch_size, src_len, d_model)\n",
    "        src_mask: mask for the source sequence\n",
    "        \"\"\"\n",
    "        # Sub-layer 1: Multi-head attention\n",
    "        attn_output = self.self_attn(src, src, src, src_mask)\n",
    "        # Residual connection and layer normalization\n",
    "        src = self.norm1(src + self.dropout(attn_output))\n",
    "\n",
    "        # Sub-layer 2: Position-wise feed-forward network\n",
    "        ff_output = self.feed_forward(src)\n",
    "        # Residual connection and layer normalization\n",
    "        src = self.norm2(src + self.dropout(ff_output))\n",
    "\n",
    "        return src\n",
    "\n",
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, d_model, h, d_ff, dropout):\n",
    "        \"\"\"\n",
    "        Initializes a single DecoderLayer.\n",
    "        \"\"\"\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.self_attn = MultiHeadAttention(d_model, h, dropout)\n",
    "        self.cross_attn = MultiHeadAttention(d_model, h, dropout)\n",
    "        self.feed_forward = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "        self.norm1 = nn.LayerNorm(d_model)\n",
    "        self.norm2 = nn.LayerNorm(d_model)\n",
    "        self.norm3 = nn.LayerNorm(d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, tgt, encoder_output, tgt_mask, src_mask):\n",
    "        \"\"\"\n",
    "        Forward pass for the DecoderLayer.\n",
    "        tgt: target tensor (batch_size, tgt_len, d_model)\n",
    "        encoder_output: output from the encoder (batch_size, src_len, d_model)\n",
    "        tgt_mask: mask for the target sequence (causal mask)\n",
    "        src_mask: mask for the source sequence\n",
    "        \"\"\"\n",
    "        # Sub-layer 1: Masked multi-head self-attention\n",
    "        attn_output = self.self_attn(tgt, tgt, tgt, tgt_mask)\n",
    "        tgt = self.norm1(tgt + self.dropout(attn_output))\n",
    "\n",
    "        # Sub-layer 2: Multi-head cross-attention\n",
    "        # Query comes from the decoder, Key and Value from the encoder\n",
    "        cross_attn_output = self.cross_attn(tgt, encoder_output, encoder_output, src_mask)\n",
    "        tgt = self.norm2(tgt + self.dropout(cross_attn_output))\n",
    "\n",
    "        # Sub-layer 3: Position-wise feed-forward network\n",
    "        ff_output = self.feed_forward(tgt)\n",
    "        tgt = self.norm3(tgt + self.dropout(ff_output))\n",
    "\n",
    "        return tgt\n",
    "\n",
    "print(\"EncoderLayer and DecoderLayer classes defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:39:58.847873Z",
     "iopub.status.busy": "2025-10-16T05:39:58.847526Z",
     "iopub.status.idle": "2025-10-16T05:39:59.177064Z",
     "shell.execute_reply": "2025-10-16T05:39:59.176271Z",
     "shell.execute_reply.started": "2025-10-16T05:39:58.847851Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Transformer model defined and instantiated successfully!\n",
      "Model Parameters: 30,082,832\n"
     ]
    }
   ],
   "source": [
    "# First, let's get our vocabulary size from the tokenizer, which is needed for the model\n",
    "vocab_size = tokenizer.get_vocab_size()\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, vocab_size, d_model, nhead, num_encoder_layers, num_decoder_layers, d_ff, dropout):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = d_model\n",
    "\n",
    "        # Embedding layers for source and target\n",
    "        self.src_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.tgt_embedding = nn.Embedding(vocab_size, d_model)\n",
    "        self.pos_encoder = PositionalEncoding(d_model, dropout=dropout)\n",
    "\n",
    "        # Encoder and Decoder stacks\n",
    "        self.encoder_layers = nn.ModuleList(\n",
    "            [EncoderLayer(d_model, nhead, d_ff, dropout) for _ in range(num_encoder_layers)]\n",
    "        )\n",
    "        self.decoder_layers = nn.ModuleList(\n",
    "            [DecoderLayer(d_model, nhead, d_ff, dropout) for _ in range(num_decoder_layers)]\n",
    "        )\n",
    "\n",
    "        # Final linear layer to project to vocab size\n",
    "        self.fc_out = nn.Linear(d_model, vocab_size)\n",
    "\n",
    "    def generate_square_subsequent_mask(self, sz):\n",
    "        \"\"\"Generates a square causal mask for the decoder.\"\"\"\n",
    "        mask = (torch.triu(torch.ones(sz, sz)) == 1).transpose(0, 1)\n",
    "        mask = mask.float().masked_fill(mask == 0, float('-inf')).masked_fill(mask == 1, float(0.0))\n",
    "        return mask\n",
    "\n",
    "    def create_padding_mask(self, seq, pad_token_id):\n",
    "        \"\"\"Creates a boolean mask for padding tokens.\"\"\"\n",
    "        # seq shape: (batch_size, seq_len)\n",
    "        # mask shape: (batch_size, 1, 1, seq_len) to be broadcastable\n",
    "        return (seq == pad_token_id).unsqueeze(1).unsqueeze(2)\n",
    "\n",
    "    def forward(self, src, tgt, pad_token_id):\n",
    "        # src shape: (batch_size, src_len)\n",
    "        # tgt shape: (batch_size, tgt_len)\n",
    "\n",
    "        # 1. Create masks\n",
    "        src_padding_mask = self.create_padding_mask(src, pad_token_id)\n",
    "        tgt_padding_mask = self.create_padding_mask(tgt, pad_token_id)\n",
    "\n",
    "        # The causal mask needs to be combined with the padding mask for the decoder's self-attention\n",
    "        tgt_len = tgt.size(1)\n",
    "        device = tgt.device\n",
    "        tgt_causal_mask = self.generate_square_subsequent_mask(tgt_len).to(device)\n",
    "\n",
    "        # The combined decoder mask prevents both looking ahead and attending to padding tokens\n",
    "        # We take the maximum of the two masks (since one is boolean and one is float)\n",
    "        # Or more simply, we can just add them since padding is 0 and causal is -inf\n",
    "        combined_tgt_mask = tgt_padding_mask | (tgt_causal_mask == float('-inf'))\n",
    "\n",
    "\n",
    "        # 2. Process inputs with embeddings and positional encoding\n",
    "        src_emb = self.pos_encoder(self.src_embedding(src) * math.sqrt(self.d_model))\n",
    "        tgt_emb = self.pos_encoder(self.tgt_embedding(tgt) * math.sqrt(self.d_model))\n",
    "\n",
    "        # 3. Pass through the encoder stack\n",
    "        encoder_output = src_emb\n",
    "        for encoder_layer in self.encoder_layers:\n",
    "            encoder_output = encoder_layer(encoder_output, src_padding_mask)\n",
    "\n",
    "        # 4. Pass through the decoder stack\n",
    "        decoder_output = tgt_emb\n",
    "        for decoder_layer in self.decoder_layers:\n",
    "            decoder_output = decoder_layer(decoder_output, encoder_output, combined_tgt_mask, src_padding_mask)\n",
    "\n",
    "        # 5. Final output layer\n",
    "        output = self.fc_out(decoder_output)\n",
    "        return output\n",
    "\n",
    "# Now, let's instantiate the model with the suggested hyperparameters from the PDF\n",
    "# Now, let's instantiate the model with the suggested hyperparameters from the PDF\n",
    "D_MODEL = 512        # Embedding dimension\n",
    "NHEAD = 2            # Number of heads\n",
    "NUM_ENCODER_LAYERS = 2\n",
    "NUM_DECODER_LAYERS = 2\n",
    "D_FF = D_MODEL * 4   # Inner feed-forward dimension, a common choice\n",
    "DROPOUT = 0.1\n",
    "\n",
    "model = Transformer(\n",
    "    vocab_size=vocab_size,\n",
    "    d_model=D_MODEL,\n",
    "    nhead=NHEAD,\n",
    "    num_encoder_layers=NUM_ENCODER_LAYERS,\n",
    "    num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "    d_ff=D_FF,\n",
    "    dropout=DROPOUT\n",
    ")\n",
    "\n",
    "print(\"Transformer model defined and instantiated successfully!\")\n",
    "# Print the total number of trainable parameters\n",
    "print(f\"Model Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:39:59.179252Z",
     "iopub.status.busy": "2025-10-16T05:39:59.178961Z",
     "iopub.status.idle": "2025-10-16T05:40:02.021889Z",
     "shell.execute_reply": "2025-10-16T05:40:02.021083Z",
     "shell.execute_reply.started": "2025-10-16T05:39:59.179235Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n",
      "Optimizer and loss function are ready.\n"
     ]
    }
   ],
   "source": [
    "# 1. Set the device to GPU if available, otherwise CPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Move the model to the selected device\n",
    "model.to(device)\n",
    "\n",
    "# 2. Define the optimizer as specified in the project PDF\n",
    "# [cite_start]Adam optimizer with betas=(0.9, 0.98) and a learning rate of 1e-4. [cite: 44, 45]\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)\n",
    "\n",
    "# 3. Define the loss function\n",
    "# We use CrossEntropyLoss, which is standard for classification tasks like predicting the next token.\n",
    "# We must ignore the padding token's index so it doesn't contribute to the loss.\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "\n",
    "print(\"Optimizer and loss function are ready.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:40:02.023280Z",
     "iopub.status.busy": "2025-10-16T05:40:02.022757Z",
     "iopub.status.idle": "2025-10-16T05:48:08.871416Z",
     "shell.execute_reply": "2025-10-16T05:48:08.870515Z",
     "shell.execute_reply.started": "2025-10-16T05:40:02.023255Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting training...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1616/1616 [00:23<00:00, 68.26it/s]\n",
      "Validating: 100%|██████████| 202/202 [00:01<00:00, 143.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 01 | Time: 0m 25s\n",
      "\tTrain Loss: 3.017 | Train PPL:  20.426\n",
      "\t Val. Loss: 2.026 |  Val. PPL:   7.585\n",
      "\t-> Saved best model (based on validation loss)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1616/1616 [00:23<00:00, 69.11it/s]\n",
      "Validating: 100%|██████████| 202/202 [00:01<00:00, 146.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 02 | Time: 0m 25s\n",
      "\tTrain Loss: 1.927 | Train PPL:   6.867\n",
      "\t Val. Loss: 1.583 |  Val. PPL:   4.871\n",
      "\t-> Saved best model (based on validation loss)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1616/1616 [00:23<00:00, 69.06it/s]\n",
      "Validating: 100%|██████████| 202/202 [00:01<00:00, 143.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 03 | Time: 0m 25s\n",
      "\tTrain Loss: 1.590 | Train PPL:   4.905\n",
      "\t Val. Loss: 1.362 |  Val. PPL:   3.904\n",
      "\t-> Saved best model (based on validation loss)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1616/1616 [00:23<00:00, 68.39it/s]\n",
      "Validating: 100%|██████████| 202/202 [00:01<00:00, 142.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 04 | Time: 0m 25s\n",
      "\tTrain Loss: 1.390 | Train PPL:   4.016\n",
      "\t Val. Loss: 1.222 |  Val. PPL:   3.393\n",
      "\t-> Saved best model (based on validation loss)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1616/1616 [00:23<00:00, 68.83it/s]\n",
      "Validating: 100%|██████████| 202/202 [00:01<00:00, 144.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 05 | Time: 0m 25s\n",
      "\tTrain Loss: 1.255 | Train PPL:   3.508\n",
      "\t Val. Loss: 1.115 |  Val. PPL:   3.050\n",
      "\t-> Saved best model (based on validation loss)\n",
      "\n",
      "Training complete.\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "from tqdm import tqdm\n",
    "\n",
    "def train_epoch(model, dataloader, optimizer, criterion, device, pad_token_id):\n",
    "    \"\"\"\n",
    "    Performs one full training epoch.\n",
    "    \"\"\"\n",
    "    model.train() # Set the model to training mode\n",
    "    total_loss = 0\n",
    "    \n",
    "    # Use tqdm for a progress bar\n",
    "    for batch in tqdm(dataloader, desc=\"Training\"):\n",
    "        # Move batch to the correct device\n",
    "        src = batch['encoder_input'].to(device)\n",
    "        tgt = batch['decoder_input'].to(device)\n",
    "        labels = batch['label'].to(device)\n",
    "\n",
    "        # Zero the gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # Forward pass - teacher forcing is used here\n",
    "        output = model(src, tgt, pad_token_id)\n",
    "        \n",
    "        # Reshape for the loss function\n",
    "        # Output: (batch_size, tgt_len, vocab_size) -> (batch_size * tgt_len, vocab_size)\n",
    "        # Labels: (batch_size, tgt_len) -> (batch_size * tgt_len)\n",
    "        output_flat = output.view(-1, output.shape[-1])\n",
    "        labels_flat = labels.view(-1)\n",
    "\n",
    "        # Calculate the loss\n",
    "        loss = criterion(output_flat, labels_flat)\n",
    "\n",
    "        # Backward pass and optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(dataloader)\n",
    "\n",
    "# def evaluate(model, dataloader, criterion, device, pad_token_id):\n",
    "#     \"\"\"\n",
    "#     Evaluates the model on the validation set.\n",
    "#     \"\"\"\n",
    "#     model.eval() # Set the model to evaluation mode\n",
    "#     total_loss = 0\n",
    "\n",
    "#     with torch.no_grad(): # No need to calculate gradients during evaluation\n",
    "#         for batch in tqdm(dataloader, desc=\"Validating\"):\n",
    "#             # Move batch to the correct device\n",
    "#             src = batch['encoder_input'].to(device)\n",
    "#             tgt = batch['decoder_input'].to(device)\n",
    "#             labels = batch['label'].to(device)\n",
    "\n",
    "#             # Forward pass\n",
    "#             output = model(src, tgt, pad_token_id)\n",
    "            \n",
    "#             # Reshape for the loss function\n",
    "#             output_flat = output.view(-1, output.shape[-1])\n",
    "#             labels_flat = labels.view(-1)\n",
    "            \n",
    "#             # Calculate the loss\n",
    "#             loss = criterion(output_flat, labels_flat)\n",
    "#             total_loss += loss.item()\n",
    "\n",
    "#     return total_loss / len(dataloader)\n",
    "\n",
    "# # --- Main Training Loop ---\n",
    "# NUM_EPOCHS = 5 # Start with a few epochs to see how it goes\n",
    "# best_val_loss = float('inf')\n",
    "\n",
    "# print(\"Starting training...\")\n",
    "\n",
    "# for epoch in range(1, NUM_EPOCHS + 1):\n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     train_loss = train_epoch(model, train_dataloader, optimizer, criterion, device, pad_token_id)\n",
    "#     val_loss = evaluate(model, val_dataloader, criterion, device, pad_token_id)\n",
    "    \n",
    "#     end_time = time.time()\n",
    "#     epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "    \n",
    "#     print(f\"\\nEpoch: {epoch:02} | Time: {epoch_mins:.0f}m {epoch_secs:.0f}s\")\n",
    "#     print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\")\n",
    "#     print(f\"\\t Val. Loss: {val_loss:.3f} |  Val. PPL: {math.exp(val_loss):7.3f}\")\n",
    "\n",
    "#     # Save the best model\n",
    "#     if val_loss < best_val_loss:\n",
    "#         best_val_loss = val_loss\n",
    "#         torch.save(model.state_dict(), 'best_model.pt')\n",
    "#         print(\"\\t-> Saved best model (based on validation loss)\")\n",
    "\n",
    "# print(\"\\nTraining complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:48:08.872718Z",
     "iopub.status.busy": "2025-10-16T05:48:08.872425Z",
     "iopub.status.idle": "2025-10-16T05:48:08.879782Z",
     "shell.execute_reply": "2025-10-16T05:48:08.879154Z",
     "shell.execute_reply.started": "2025-10-16T05:48:08.872694Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol, device):\n",
    "    src = src.to(device)\n",
    "    src_mask = src_mask.to(device)\n",
    "\n",
    "    with torch.no_grad():\n",
    "        encoder_output = model.src_embedding(src) * math.sqrt(D_MODEL)\n",
    "        encoder_output = model.pos_encoder(encoder_output)\n",
    "        for layer in model.encoder_layers:\n",
    "            encoder_output = layer(encoder_output, src_mask)\n",
    "\n",
    "    # Start with the [BOS] token\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type(torch.long).to(device)\n",
    "\n",
    "    for i in range(max_len - 1):\n",
    "        with torch.no_grad():\n",
    "            tgt_mask = (model.generate_square_subsequent_mask(ys.size(1))\n",
    "                        .type(torch.bool)).to(device)\n",
    "\n",
    "            tgt_emb = model.tgt_embedding(ys) * math.sqrt(D_MODEL)\n",
    "            tgt_emb = model.pos_encoder(tgt_emb)\n",
    "\n",
    "            decoder_output = tgt_emb\n",
    "            for layer in model.decoder_layers:\n",
    "                decoder_output = layer(decoder_output, encoder_output, tgt_mask, None) # No src_mask needed in cross-attn here\n",
    "\n",
    "            prob = model.fc_out(decoder_output[:, -1])\n",
    "            _, next_word = torch.max(prob, dim=1)\n",
    "            next_word = next_word.item()\n",
    "\n",
    "        ys = torch.cat([ys, torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "        if next_word == eos_token_id:\n",
    "            break\n",
    "    return ys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:49:23.019847Z",
     "iopub.status.busy": "2025-10-16T05:49:23.019569Z",
     "iopub.status.idle": "2025-10-16T05:49:29.340980Z",
     "shell.execute_reply": "2025-10-16T05:49:29.340176Z",
     "shell.execute_reply.started": "2025-10-16T05:49:23.019828Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed sacrebleu and rouge-score.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  DEPRECATION: Building 'rouge-score' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'rouge-score'. Discussion can be found at https://github.com/pypa/pip/issues/6334\n"
     ]
    }
   ],
   "source": [
    "!pip install sacrebleu rouge-score -q\n",
    "print(\"Installed sacrebleu and rouge-score.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:49:39.995820Z",
     "iopub.status.busy": "2025-10-16T05:49:39.995518Z",
     "iopub.status.idle": "2025-10-16T05:49:40.542911Z",
     "shell.execute_reply": "2025-10-16T05:49:40.542289Z",
     "shell.execute_reply.started": "2025-10-16T05:49:39.995798Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import sacrebleu\n",
    "from rouge_score import rouge_scorer\n",
    "\n",
    "def evaluate(model, dataloader, criterion, tokenizer, device, pad_token_id, bos_token_id, eos_token_id):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    \n",
    "    all_predictions = []\n",
    "    all_references = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in tqdm(dataloader, desc=\"Validating\"):\n",
    "            src = batch['encoder_input'].to(device)\n",
    "            tgt = batch['decoder_input'].to(device)\n",
    "            labels = batch['label'].to(device)\n",
    "\n",
    "            # --- Loss Calculation (same as before) ---\n",
    "            output = model(src, tgt, pad_token_id)\n",
    "            output_flat = output.view(-1, output.shape[-1])\n",
    "            labels_flat = labels.view(-1)\n",
    "            loss = criterion(output_flat, labels_flat)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # --- Metric Calculation ---\n",
    "            # Generate predictions for each item in the batch\n",
    "            for i in range(src.size(0)):\n",
    "                input_seq = src[i:i+1] # Shape: (1, seq_len)\n",
    "                label_seq = labels[i]  # Shape: (seq_len)\n",
    "\n",
    "                src_padding_mask = model.create_padding_mask(input_seq, pad_token_id)\n",
    "                \n",
    "                # Generate a prediction using greedy decoding\n",
    "                prediction_ids = greedy_decode(model, input_seq, src_padding_mask, max_len=50, start_symbol=bos_token_id, device=device)\n",
    "                \n",
    "                # Decode prediction and label back to text\n",
    "                pred_text = tokenizer.decode(prediction_ids.squeeze(0).tolist(), skip_special_tokens=True)\n",
    "                ref_text = tokenizer.decode(label_seq.tolist(), skip_special_tokens=True)\n",
    "                \n",
    "                all_predictions.append(pred_text)\n",
    "                all_references.append([ref_text]) # sacrebleu expects a list of references\n",
    "\n",
    "    # Calculate metrics over the entire validation set\n",
    "    bleu = sacrebleu.corpus_bleu(all_predictions, all_references)\n",
    "    chrf = sacrebleu.corpus_chrf(all_predictions, all_references)\n",
    "    \n",
    "    # ROUGE-L\n",
    "    scorer = rouge_scorer.RougeScorer(['rougeL'], use_stemmer=True)\n",
    "    rouge_l_sum = 0\n",
    "    for pred, ref in zip(all_predictions, all_references):\n",
    "        scores = scorer.score(ref[0], pred)\n",
    "        rouge_l_sum += scores['rougeL'].fmeasure\n",
    "    rouge_l_avg = rouge_l_sum / len(all_predictions)\n",
    "\n",
    "    return total_loss / len(dataloader), bleu.score, chrf.score, rouge_l_avg * 100\n",
    "\n",
    "\n",
    "# # --- New Main Training Loop ---\n",
    "# NUM_EPOCHS = 10 # Let's train for a bit longer\n",
    "# best_bleu_score = -1.0\n",
    "\n",
    "# print(\"\\nStarting training with full metric evaluation...\")\n",
    "\n",
    "# for epoch in range(1, NUM_EPOCHS + 1):\n",
    "#     start_time = time.time()\n",
    "    \n",
    "#     train_loss = train_epoch(model, train_dataloader, optimizer, criterion, device, pad_token_id)\n",
    "#     val_loss, bleu, chrf, rouge_l = evaluate(model, val_dataloader, criterion, tokenizer, device, pad_token_id, bos_token_id, eos_token_id)\n",
    "    \n",
    "#     end_time = time.time()\n",
    "#     epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "    \n",
    "#     print(f\"\\nEpoch: {epoch:02} | Time: {epoch_mins:.0f}m {epoch_secs:.0f}s\")\n",
    "#     print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\")\n",
    "#     print(f\"\\t Val. Loss: {val_loss:.3f} |  Val. PPL: {math.exp(val_loss):7.3f}\")\n",
    "#     print(f\"\\t Val. BLEU: {bleu:.2f} | ROUGE-L: {rouge_l:.2f} | chrF: {chrf:.2f}\")\n",
    "\n",
    "#     # Save the best model based on BLEU score\n",
    "#     if bleu > best_bleu_score:\n",
    "#         best_bleu_score = bleu\n",
    "#         torch.save(model.state_dict(), 'best_model_bleu.pt')\n",
    "#         print(f\"\\t-> New best model saved with BLEU score: {bleu:.2f}\")\n",
    "\n",
    "# print(\"\\nTraining complete.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:49:45.809463Z",
     "iopub.status.busy": "2025-10-16T05:49:45.808494Z",
     "iopub.status.idle": "2025-10-16T05:49:45.818101Z",
     "shell.execute_reply": "2025-10-16T05:49:45.817321Z",
     "shell.execute_reply.started": "2025-10-16T05:49:45.809438Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "def beam_search_decode(model, src, max_len, start_symbol, device, beam_size=3):\n",
    "    \"\"\"\n",
    "    Performs beam search decoding.\n",
    "    \"\"\"\n",
    "    # Move input to device and create masks\n",
    "    src = src.to(device)\n",
    "    src_padding_mask = model.create_padding_mask(src, pad_token_id)\n",
    "\n",
    "    # --- Step 1: Encode the input sequence ---\n",
    "    with torch.no_grad():\n",
    "        encoder_output = model.src_embedding(src) * math.sqrt(D_MODEL)\n",
    "        encoder_output = model.pos_encoder(encoder_output)\n",
    "        for layer in model.encoder_layers:\n",
    "            encoder_output = layer(encoder_output, src_padding_mask)\n",
    "\n",
    "    # --- Step 2: Start the beam search ---\n",
    "    # Start with the [BOS] token. Beams are stored as (sequence, score).\n",
    "    # Scores are log probabilities, so they are negative and we want to maximize them.\n",
    "    beams = [(torch.tensor([start_symbol], device=device).unsqueeze(0), 0.0)]\n",
    "\n",
    "    # --- Step 3: Loop until max_len or all beams end with [EOS] ---\n",
    "    for _ in range(max_len - 1):\n",
    "        new_beams = []\n",
    "        all_candidates_ended = True\n",
    "\n",
    "        for seq, score in beams:\n",
    "            # If the last token is [EOS], this sequence is finished.\n",
    "            if seq[0, -1].item() == eos_token_id:\n",
    "                new_beams.append((seq, score))\n",
    "                continue\n",
    "            \n",
    "            all_candidates_ended = False\n",
    "\n",
    "            # --- Get predictions for the next word ---\n",
    "            with torch.no_grad():\n",
    "                tgt_mask = model.generate_square_subsequent_mask(seq.size(1)).to(device)\n",
    "                tgt_emb = model.tgt_embedding(seq) * math.sqrt(D_MODEL)\n",
    "                tgt_emb = model.pos_encoder(tgt_emb)\n",
    "\n",
    "                decoder_output = tgt_emb\n",
    "                for layer in model.decoder_layers:\n",
    "                    decoder_output = layer(decoder_output, encoder_output, tgt_mask, None)\n",
    "\n",
    "                # Get the log probabilities for the next word\n",
    "                logits = model.fc_out(decoder_output[:, -1])\n",
    "                log_probs = F.log_softmax(logits, dim=-1)\n",
    "\n",
    "            # --- Find the top k words for the current beam ---\n",
    "            top_log_probs, top_indices = torch.topk(log_probs, beam_size, dim=1)\n",
    "\n",
    "            # --- Create new candidate sequences ---\n",
    "            for i in range(beam_size):\n",
    "                next_word_idx = top_indices[0, i].item()\n",
    "                next_word_log_prob = top_log_probs[0, i].item()\n",
    "\n",
    "                new_seq = torch.cat([seq, torch.tensor([[next_word_idx]], device=device)], dim=1)\n",
    "                new_score = score + next_word_log_prob\n",
    "                new_beams.append((new_seq, new_score))\n",
    "\n",
    "        if all_candidates_ended:\n",
    "            break\n",
    "            \n",
    "        # --- Step 4: Prune the beams ---\n",
    "        # Sort all new candidates by their score and keep only the top k\n",
    "        beams = sorted(new_beams, key=lambda x: x[1], reverse=True)[:beam_size]\n",
    "\n",
    "    # Return the sequence from the best-scoring beam\n",
    "    best_seq, _ = beams[0]\n",
    "    return best_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-10-16T05:49:50.761624Z",
     "iopub.status.busy": "2025-10-16T05:49:50.760960Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Starting training with a larger model (d_model=512)...\n",
      "Model Parameters: 30,082,832\n",
      "\n",
      "Starting training with more epochs and LR scheduler...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1616/1616 [00:23<00:00, 67.93it/s]\n",
      "Validating: 100%|██████████| 202/202 [08:06<00:00,  2.41s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 01 | Time: 8m 32s\n",
      "\tCurrent LR: 0.000100\n",
      "\tTrain Loss: 3.032 | Train PPL:  20.732\n",
      "\t Val. Loss: 2.021 |  Val. PPL:   7.548\n",
      "\t Val. BLEU: 2.02 | ROUGE-L: 1.25 | chrF: 9.03\n",
      "\t-> New best model saved with BLEU score: 2.02\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1616/1616 [00:23<00:00, 68.55it/s]\n",
      "Validating: 100%|██████████| 202/202 [08:09<00:00,  2.43s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 02 | Time: 8m 35s\n",
      "\tCurrent LR: 0.000100\n",
      "\tTrain Loss: 1.935 | Train PPL:   6.922\n",
      "\t Val. Loss: 1.564 |  Val. PPL:   4.776\n",
      "\t Val. BLEU: 2.02 | ROUGE-L: 1.25 | chrF: 9.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1616/1616 [00:23<00:00, 67.97it/s]\n",
      "Validating: 100%|██████████| 202/202 [07:39<00:00,  2.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 03 | Time: 8m 5s\n",
      "\tCurrent LR: 0.000100\n",
      "\tTrain Loss: 1.601 | Train PPL:   4.960\n",
      "\t Val. Loss: 1.349 |  Val. PPL:   3.854\n",
      "\t Val. BLEU: 2.02 | ROUGE-L: 1.25 | chrF: 9.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1616/1616 [00:23<00:00, 68.34it/s]\n",
      "Validating: 100%|██████████| 202/202 [07:42<00:00,  2.29s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 04 | Time: 8m 7s\n",
      "\tCurrent LR: 0.000100\n",
      "\tTrain Loss: 1.400 | Train PPL:   4.054\n",
      "\t Val. Loss: 1.217 |  Val. PPL:   3.376\n",
      "\t Val. BLEU: 2.02 | ROUGE-L: 1.25 | chrF: 9.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1616/1616 [00:23<00:00, 68.59it/s]\n",
      "Validating: 100%|██████████| 202/202 [08:25<00:00,  2.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 05 | Time: 8m 50s\n",
      "\tCurrent LR: 0.000100\n",
      "\tTrain Loss: 1.262 | Train PPL:   3.532\n",
      "\t Val. Loss: 1.125 |  Val. PPL:   3.079\n",
      "\t Val. BLEU: 2.02 | ROUGE-L: 1.25 | chrF: 9.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1616/1616 [00:23<00:00, 68.73it/s]\n",
      "Validating: 100%|██████████| 202/202 [08:47<00:00,  2.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 06 | Time: 9m 13s\n",
      "\tCurrent LR: 0.000100\n",
      "\tTrain Loss: 1.161 | Train PPL:   3.194\n",
      "\t Val. Loss: 1.040 |  Val. PPL:   2.829\n",
      "\t Val. BLEU: 1.27 | ROUGE-L: 2.08 | chrF: 9.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1616/1616 [00:23<00:00, 68.16it/s]\n",
      "Validating: 100%|██████████| 202/202 [07:38<00:00,  2.27s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 07 | Time: 8m 3s\n",
      "\tCurrent LR: 0.000100\n",
      "\tTrain Loss: 1.077 | Train PPL:   2.934\n",
      "\t Val. Loss: 1.002 |  Val. PPL:   2.724\n",
      "\t Val. BLEU: 13.54 | ROUGE-L: 4.75 | chrF: 12.40\n",
      "\t-> New best model saved with BLEU score: 13.54\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1616/1616 [00:23<00:00, 68.29it/s]\n",
      "Validating: 100%|██████████| 202/202 [07:56<00:00,  2.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 08 | Time: 8m 21s\n",
      "\tCurrent LR: 0.000100\n",
      "\tTrain Loss: 1.015 | Train PPL:   2.759\n",
      "\t Val. Loss: 0.951 |  Val. PPL:   2.588\n",
      "\t Val. BLEU: 13.54 | ROUGE-L: 4.75 | chrF: 12.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1616/1616 [00:22<00:00, 70.51it/s]\n",
      "Validating: 100%|██████████| 202/202 [08:30<00:00,  2.53s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 09 | Time: 8m 54s\n",
      "\tCurrent LR: 0.000100\n",
      "\tTrain Loss: 0.958 | Train PPL:   2.608\n",
      "\t Val. Loss: 0.920 |  Val. PPL:   2.510\n",
      "\t Val. BLEU: 13.54 | ROUGE-L: 4.75 | chrF: 12.40\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1616/1616 [00:23<00:00, 70.18it/s]\n",
      "Validating: 100%|██████████| 202/202 [08:43<00:00,  2.59s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 10 | Time: 9m 9s\n",
      "\tCurrent LR: 0.000100\n",
      "\tTrain Loss: 0.913 | Train PPL:   2.493\n",
      "\t Val. Loss: 0.892 |  Val. PPL:   2.440\n",
      "\t Val. BLEU: 1.80 | ROUGE-L: 1.10 | chrF: 12.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1616/1616 [00:23<00:00, 70.05it/s]\n",
      "Validating: 100%|██████████| 202/202 [08:35<00:00,  2.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 11 | Time: 9m 2s\n",
      "\tCurrent LR: 0.000100\n",
      "\tTrain Loss: 0.876 | Train PPL:   2.401\n",
      "\t Val. Loss: 0.870 |  Val. PPL:   2.387\n",
      "\t Val. BLEU: 1.80 | ROUGE-L: 1.10 | chrF: 12.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1616/1616 [00:22<00:00, 70.39it/s]\n",
      "Validating: 100%|██████████| 202/202 [08:03<00:00,  2.39s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 12 | Time: 8m 29s\n",
      "\tCurrent LR: 0.000100\n",
      "\tTrain Loss: 0.840 | Train PPL:   2.316\n",
      "\t Val. Loss: 0.842 |  Val. PPL:   2.322\n",
      "\t Val. BLEU: 1.80 | ROUGE-L: 1.10 | chrF: 12.60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1616/1616 [00:23<00:00, 69.80it/s]\n",
      "Validating: 100%|██████████| 202/202 [07:14<00:00,  2.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 13 | Time: 7m 39s\n",
      "\tCurrent LR: 0.000100\n",
      "\tTrain Loss: 0.813 | Train PPL:   2.255\n",
      "\t Val. Loss: 0.839 |  Val. PPL:   2.314\n",
      "\t Val. BLEU: 1.80 | ROUGE-L: 1.26 | chrF: 11.33\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1616/1616 [00:22<00:00, 70.31it/s]\n",
      "Validating: 100%|██████████| 202/202 [08:26<00:00,  2.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 14 | Time: 8m 52s\n",
      "\tCurrent LR: 0.000100\n",
      "\tTrain Loss: 0.778 | Train PPL:   2.178\n",
      "\t Val. Loss: 0.816 |  Val. PPL:   2.262\n",
      "\t Val. BLEU: 2.66 | ROUGE-L: 1.42 | chrF: 13.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training: 100%|██████████| 1616/1616 [00:22<00:00, 70.45it/s]\n",
      "Validating: 100%|██████████| 202/202 [07:24<00:00,  2.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 15 | Time: 7m 50s\n",
      "\tCurrent LR: 0.000100\n",
      "\tTrain Loss: 0.757 | Train PPL:   2.132\n",
      "\t Val. Loss: 0.814 |  Val. PPL:   2.256\n",
      "\t Val. BLEU: 2.57 | ROUGE-L: 1.42 | chrF: 13.15\n",
      "\n",
      "Improved training complete.\n"
     ]
    }
   ],
   "source": [
    "# from torch.optim.lr_scheduler import StepLR\n",
    "\n",
    "# --- Re-initialize Model and Optimizer ---\n",
    "# We re-initialize the model to train it from scratch with the new strategy\n",
    "model = Transformer(\n",
    "    vocab_size=vocab_size, d_model=D_MODEL, nhead=NHEAD,\n",
    "    num_encoder_layers=NUM_ENCODER_LAYERS, num_decoder_layers=NUM_DECODER_LAYERS,\n",
    "    d_ff=D_FF, dropout=DROPOUT\n",
    ").to(device)\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=1e-4, betas=(0.9, 0.98), eps=1e-9)\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=pad_token_id)\n",
    "\n",
    "# --- New Main Training Loop ---\n",
    "NUM_EPOCHS = 15  # Train for more epochs (Method 2)\n",
    "best_bleu_score = -1.0\n",
    "\n",
    "print(f\"\\nStarting training with a larger model (d_model={D_MODEL})...\")\n",
    "print(f\"Model Parameters: {sum(p.numel() for p in model.parameters() if p.requires_grad):,}\")\n",
    "\n",
    "print(\"\\nStarting training with more epochs and LR scheduler...\")\n",
    "\n",
    "for epoch in range(1, NUM_EPOCHS + 1):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    train_loss = train_epoch(model, train_dataloader, optimizer, criterion, device, pad_token_id)\n",
    "    val_loss, bleu, chrf, rouge_l = evaluate(model, val_dataloader, criterion, tokenizer, device, pad_token_id, bos_token_id, eos_token_id)\n",
    "    \n",
    "    # --- Step the scheduler ---\n",
    "    # scheduler.step()\n",
    "\n",
    "    end_time = time.time()\n",
    "    epoch_mins, epoch_secs = divmod(end_time - start_time, 60)\n",
    "    \n",
    "    print(f\"\\nEpoch: {epoch:02} | Time: {epoch_mins:.0f}m {epoch_secs:.0f}s\")\n",
    "    print(f\"\\tCurrent LR: {optimizer.param_groups[0]['lr']:.6f}\") # Display current learning rate\n",
    "    print(f\"\\tTrain Loss: {train_loss:.3f} | Train PPL: {math.exp(train_loss):7.3f}\")\n",
    "    print(f\"\\t Val. Loss: {val_loss:.3f} |  Val. PPL: {math.exp(val_loss):7.3f}\")\n",
    "    print(f\"\\t Val. BLEU: {bleu:.2f} | ROUGE-L: {rouge_l:.2f} | chrF: {chrf:.2f}\")\n",
    "\n",
    "    # Save the best model based on BLEU score\n",
    "    if bleu > best_bleu_score:\n",
    "        best_bleu_score = bleu\n",
    "        # Let's give this improved model a new name\n",
    "        torch.save(model.state_dict(), 'best_model_improved.pt')\n",
    "        print(f\"\\t-> New best model saved with BLEU score: {bleu:.2f}\")\n",
    "\n",
    "print(\"\\nImproved training complete.\")"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1962861,
     "sourceId": 3238154,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31153,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
